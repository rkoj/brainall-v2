# BrainAll V2 - Arquitetura Multi-Modelo Escal√°vel

**Data:** 15 Novembro 2025  
**Vers√£o:** 2.0  
**Status:** Planeamento e Design

---

## üìã √çNDICE

1. [Contexto e An√°lise](#contexto-e-an√°lise)
2. [Infraestrutura Dispon√≠vel](#infraestrutura-dispon√≠vel)
3. [Compara√ß√£o: Brain All V1 vs BrainAll V2](#compara√ß√£o)
4. [Arquitetura Proposta](#arquitetura-proposta)
5. [Stack Tecnol√≥gica](#stack-tecnol√≥gica)
6. [Distribui√ß√£o de Servi√ßos](#distribui√ß√£o-de-servi√ßos)
7. [Roadmap de Implementa√ß√£o](#roadmap)

---

## üéØ CONTEXTO E AN√ÅLISE

### Projeto Anterior: Brain All V1

**O que foi:**
- Agente AI aut√≥nomo privado para infraestrutura GPU e DevOps
- LLM local (Llama 3.1 70B via Ollama)
- Execu√ß√£o de c√≥digo remoto (Python, Shell, Node.js)
- Mem√≥ria persistente com RAG (ChromaDB)
- Agent loop com planeamento aut√≥nomo
- Interface 3 colunas estilo Manus

**Problemas:**
- ‚ùå Disco cheio (1.5TB de mem√≥ria ChromaDB corrompida)
- ‚ùå Apenas 1 modelo LLM (Llama 3.1 70B)
- ‚ùå Sem multi-modelo support
- ‚ùå Infraestrutura n√£o dimensionada corretamente
- ‚ùå Sem load balancing ou redund√¢ncia

**Sucessos:**
- ‚úÖ Agente funcionou perfeitamente
- ‚úÖ RAG e mem√≥ria funcionaram bem
- ‚úÖ Execu√ß√£o remota via SSH est√°vel
- ‚úÖ Interface bonita e funcional
- ‚úÖ Teste de aprendizagem aut√≥noma bem-sucedido

---

### Novo Projeto: BrainAll V2 (page-navigator)

**O que √©:**
- Sistema de chat AI multi-modelo
- Frontend moderno (React + Vite + shadcn/ui)
- Suporte para m√∫ltiplos LLMs (OpenAI, Anthropic, Llama, Mistral, etc.)
- Upload e processamento de ficheiros
- Grava√ß√£o e transcri√ß√£o de √°udio
- Modo Agente com ferramentas
- Hist√≥rico de conversas persistente

**Estado Atual:**
- ‚úÖ Frontend completo (apenas mock responses)
- ‚ùå Backend n√£o implementado
- ‚ùå Sem integra√ß√£o com LLMs
- ‚ùå Sem base de dados

---

## üñ•Ô∏è INFRAESTRUTURA DISPON√çVEL

### 1. GPU Server (Hetzner Finl√¢ndia)
**Hostname:** gpu-node-130  
**IP:** 65.21.33.83  
**Specs:**
- **GPU:** NVIDIA RTX 6000 Ada Generation
  - VRAM: 49GB (48.14GB us√°vel)
  - CUDA: 12.8
  - Driver: 570.195.03
- **CPU:** AMD EPYC (detalhes a confirmar)
- **RAM:** ~128GB (estimado)
- **Disco:** 1.8TB NVMe
  - Usado: 110GB (7%)
  - Livre: 1.6TB (93%)
- **OS:** Ubuntu 22.04 (6.8.0-85-generic)
- **Rede:** vSwitch VLAN 4000 (192.168.100.x)

**Credenciais:**
- User: root
- Password: Cl@$$UNDER2025
- SSH: `ssh root@65.21.33.83`

**Estado Atual:**
- ‚úÖ GPU funcional e limpa
- ‚úÖ Disco com espa√ßo abundante
- ‚úÖ Drivers NVIDIA atualizados
- ‚ùå Brain All V1 parado (processo terminado)
- ‚ùå Ollama status desconhecido

---

### 2. Servidor AX102 #1 (Hetzner Finl√¢ndia)
**Specs:**
- **CPU:** AMD Ryzen 9 7950X
  - 16 cores / 32 threads
  - Base: 4.5 GHz, Boost: 5.7 GHz
- **RAM:** 128 GB DDR5-4800
- **Storage:** 2x 3.84 TB NVMe SSD (RAID poss√≠vel)
- **Network:** 1 Gbit/s
- **OS:** Proxmox VE (host)

**Uso Proposto:**
- Gateway API (Node.js + Express + tRPC)
- Nginx (load balancer + SSL)
- WebSocket server (streaming)
- Auth service
- File upload handler

---

### 3. Servidor AX102 #2 (Hetzner Finl√¢ndia)
**Specs:**
- **CPU:** AMD Ryzen 9 7950X (16c/32t)
- **RAM:** 128 GB DDR5-4800
- **Storage:** 2x 3.84 TB NVMe SSD
- **Network:** 1 Gbit/s
- **OS:** Proxmox VE (host)

**Uso Proposto:**
- AI Service (Python FastAPI)
- Processing workers (BullMQ/Celery)
- Whisper (transcri√ß√£o de √°udio)
- Image processing
- PDF extraction
- Web scraping (modo agente)

---

### 4. Servidor EK41 (Hetzner Finl√¢ndia)
**Specs:**
- **CPU:** AMD EPYC 7502P
  - 32 cores / 64 threads
  - Base: 2.5 GHz, Boost: 3.35 GHz
- **RAM:** 128 GB DDR4-3200
- **Storage:** 2x 960 GB NVMe SSD (RAID 1 recomendado)
- **Network:** 1 Gbit/s
- **OS:** Proxmox VE (host)

**Uso Proposto:**
- PostgreSQL 16 (database principal)
- Redis 7 (cache + queue)
- MinIO (S3-compatible storage)
- Backup service
- Monitoring (Prometheus + Grafana)

---

### 5. VCloud
**Specs:** A definir  
**Uso Proposto:** Frontend hosting ou redund√¢ncia

---

### 6. Rede (vSwitch VLAN 4000)
**Configura√ß√£o:**
- Subnet: 192.168.100.0/24
- Membros: GPU + AX102 #1 + AX102 #2 + EK41
- Lat√™ncia: <1ms entre servidores
- Bandwidth: 10 Gbit/s interno

---

## üîÑ COMPARA√á√ÉO: Brain All V1 vs BrainAll V2

| Aspecto | Brain All V1 | BrainAll V2 (Proposto) |
|---------|--------------|------------------------|
| **Arquitetura** | Monol√≠tico (GPU √∫nica) | Distribu√≠da (4 servidores) |
| **LLMs** | 1 modelo (Llama 70B) | Multi-modelo (5+) |
| **Inference** | Ollama | vLLM (OpenAI-compatible) |
| **Backend** | Python FastAPI | H√≠brido (Node.js + Python) |
| **Frontend** | Custom (Fase 4) | React + Vite (Lovable) |
| **Database** | ChromaDB (vector only) | PostgreSQL + Redis + ChromaDB |
| **Storage** | Local filesystem | MinIO (S3-compatible) |
| **Mem√≥ria** | ChromaDB (1.5TB!) | PostgreSQL + Vector search |
| **Escalabilidade** | Vertical (GPU) | Horizontal (4 servers) |
| **Redund√¢ncia** | Nenhuma | Load balancing + failover |
| **Monitoring** | Logs b√°sicos | Prometheus + Grafana |
| **Deployment** | Manual | Docker + CI/CD |

---

## üèóÔ∏è ARQUITETURA PROPOSTA

### Vis√£o Geral

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         USERS / CLIENTS                          ‚îÇ
‚îÇ                    (Web Browser / Mobile)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ HTTPS/WSS
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      CLOUDFLARE / CDN                            ‚îÇ
‚îÇ                    (SSL, DDoS Protection)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FRONTEND (VCloud/Vercel)                      ‚îÇ
‚îÇ                  - React 18 + TypeScript                         ‚îÇ
‚îÇ                  - Vite 5 + Tailwind CSS                         ‚îÇ
‚îÇ                  - shadcn/ui components                          ‚îÇ
‚îÇ                  - page-navigator codebase                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ HTTPS/WSS
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              LOAD BALANCER (Nginx on AX102 #1)                   ‚îÇ
‚îÇ              - SSL Termination                                   ‚îÇ
‚îÇ              - Rate Limiting                                     ‚îÇ
‚îÇ              - WebSocket Upgrade                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                             ‚îÇ
            ‚ñº                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Gateway API #1      ‚îÇ    ‚îÇ   Gateway API #2      ‚îÇ
‚îÇ   (AX102 #1 VM)       ‚îÇ    ‚îÇ   (AX102 #2 VM)       ‚îÇ
‚îÇ   - Node.js 22        ‚îÇ    ‚îÇ   - Node.js 22        ‚îÇ
‚îÇ   - Express + tRPC    ‚îÇ    ‚îÇ   - Express + tRPC    ‚îÇ
‚îÇ   - Socket.io         ‚îÇ    ‚îÇ   - Socket.io         ‚îÇ
‚îÇ   - JWT Auth          ‚îÇ    ‚îÇ   - JWT Auth          ‚îÇ
‚îÇ   - File Upload       ‚îÇ    ‚îÇ   - File Upload       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                             ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ              ‚îÇ              ‚îÇ
            ‚ñº              ‚ñº              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI Service      ‚îÇ ‚îÇ Processing       ‚îÇ ‚îÇ GPU Inference    ‚îÇ
‚îÇ  (AX102 #2 VM)   ‚îÇ ‚îÇ Workers          ‚îÇ ‚îÇ (GPU Server)     ‚îÇ
‚îÇ  - Python FastAPI‚îÇ ‚îÇ (AX102 #2 VM)    ‚îÇ ‚îÇ - vLLM           ‚îÇ
‚îÇ  - LangChain     ‚îÇ ‚îÇ - Celery/BullMQ  ‚îÇ ‚îÇ - Multi-model    ‚îÇ
‚îÇ  - Model Router  ‚îÇ ‚îÇ - Whisper        ‚îÇ ‚îÇ - OpenAI API     ‚îÇ
‚îÇ  - Tool Executor ‚îÇ ‚îÇ - Image Proc     ‚îÇ ‚îÇ - Model Cache    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ              ‚îÇ                        ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ   Database & Storage (EK41)  ‚îÇ
            ‚îÇ   - PostgreSQL 16            ‚îÇ
            ‚îÇ   - Redis 7                  ‚îÇ
            ‚îÇ   - MinIO (S3)               ‚îÇ
            ‚îÇ   - pgvector (embeddings)    ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Fluxo de Dados: Chat Request

```
1. User ‚Üí Frontend: "Explica-me o que √© RAG"
2. Frontend ‚Üí Gateway API: POST /api/chat
3. Gateway ‚Üí Auth: Valida JWT token
4. Gateway ‚Üí AI Service: Forward request
5. AI Service ‚Üí Database: Busca hist√≥rico + contexto
6. AI Service ‚Üí GPU Inference: POST /v1/chat/completions
7. GPU (vLLM) ‚Üí Model: Llama 3.1 70B inference
8. Model ‚Üí GPU: Streaming response
9. GPU ‚Üí AI Service: SSE stream
10. AI Service ‚Üí Gateway: WebSocket stream
11. Gateway ‚Üí Frontend: Display streaming
12. AI Service ‚Üí Database: Save conversation
```

---

### Fluxo de Dados: Modo Agente

```
1. User ‚Üí Frontend: "Cria um script Python que..."
2. Frontend ‚Üí Gateway: POST /api/agent/task
3. Gateway ‚Üí AI Service: Agent task
4. AI Service ‚Üí LLM: Plan task (decompose)
5. LLM ‚Üí AI Service: [Step 1, Step 2, Step 3]
6. AI Service ‚Üí Processing Worker: Execute steps
7. Worker ‚Üí Sandbox: SSH execute code
8. Sandbox ‚Üí Worker: Output + status
9. Worker ‚Üí AI Service: Results
10. AI Service ‚Üí LLM: Analyze results
11. LLM ‚Üí AI Service: Next action
12. (Loop until task complete)
13. AI Service ‚Üí Gateway ‚Üí Frontend: Final result
```

---

## üõ†Ô∏è STACK TECNOL√ìGICA

### Frontend
```yaml
Framework: React 18 + TypeScript
Build: Vite 5
Styling: Tailwind CSS 3
Components: shadcn/ui
Routing: React Router 6
State: TanStack Query 5
WebSocket: native WebSocket API
Forms: React Hook Form + Zod
```

### Gateway API (Node.js)
```yaml
Runtime: Node.js 22 LTS
Framework: Express 4 ou Fastify 4
API: tRPC 11 (type-safe)
WebSocket: Socket.io 4
Auth: JWT (jose library)
Validation: Zod
ORM: Drizzle ORM
Database Client: node-postgres
Cache: ioredis
File Upload: multer
```

### AI Service (Python)
```yaml
Runtime: Python 3.11
Framework: FastAPI 0.104+
Async: asyncio + uvicorn
LLM: OpenAI SDK (multi-provider)
Orchestration: LangChain 0.1+
Embeddings: sentence-transformers
Vector DB: pgvector (PostgreSQL)
Queue: Celery 5 + Redis
HTTP Client: httpx
```

### GPU Inference
```yaml
Server: vLLM 0.2+ (OpenAI-compatible)
Models:
  - Llama 3.1 70B (quantized 4-bit) ~24GB
  - Mistral 7B Instruct (FP16) ~14GB
  - Phi-3 Medium (FP16) ~8GB
  - CodeLlama 34B (quantized) ~18GB
API: OpenAI-compatible endpoints
Quantization: AWQ ou GPTQ
```

### Database & Storage
```yaml
Database: PostgreSQL 16
  - pgvector extension (embeddings)
  - Full-text search
  - JSONB for metadata
Cache: Redis 7
  - Session storage
  - Queue backend
  - Rate limiting
Storage: MinIO (S3-compatible)
  - File uploads
  - Model cache
  - Backups
```

### DevOps & Monitoring
```yaml
Containers: Docker 24 + Docker Compose
Orchestration: Proxmox VE (VMs)
Reverse Proxy: Nginx 1.24 + Caddy 2
SSL: Let's Encrypt (auto-renewal)
Monitoring: Prometheus + Grafana
Logs: Loki + Promtail
Metrics: node_exporter, nvidia_gpu_exporter
Backup: restic + S3
```

---

## üì¶ DISTRIBUI√á√ÉO DE SERVI√áOS

### GPU Server (65.21.33.83)

**Servi√ßos:**
```yaml
1. vLLM Inference Server
   - Port: 8000
   - Models: 4 simult√¢neos
   - VRAM: ~46GB usado
   - API: OpenAI-compatible

2. Model Cache
   - Path: /models
   - Size: ~100GB
   - Format: safetensors

3. Monitoring
   - nvidia_gpu_exporter
   - node_exporter
```

**Configura√ß√£o de Modelos:**
```python
# vLLM config
models = [
    {
        "name": "llama-3.1-70b-instruct",
        "path": "/models/llama-3.1-70b-awq",
        "vram": 24,  # GB
        "quantization": "awq",
    },
    {
        "name": "mistral-7b-instruct",
        "path": "/models/mistral-7b-v0.2",
        "vram": 14,
        "quantization": "fp16",
    },
    {
        "name": "phi-3-medium",
        "path": "/models/phi-3-medium-128k",
        "vram": 8,
        "quantization": "fp16",
    },
]
```

---

### AX102 #1 (Proxmox Host)

**VMs:**
```yaml
VM 1: Gateway API Primary
  - vCPU: 8 cores
  - RAM: 32GB
  - Disk: 200GB
  - Services:
    - Nginx (80, 443)
    - Node.js API (3000)
    - Socket.io (3001)

VM 2: Gateway API Secondary (Standby)
  - vCPU: 4 cores
  - RAM: 16GB
  - Disk: 100GB
  - Services: Same as VM1
```

---

### AX102 #2 (Proxmox Host)

**VMs:**
```yaml
VM 1: AI Service
  - vCPU: 12 cores
  - RAM: 48GB
  - Disk: 300GB
  - Services:
    - FastAPI (8001)
    - LangChain
    - Model Router

VM 2: Processing Workers
  - vCPU: 8 cores
  - RAM: 32GB
  - Disk: 200GB
  - Services:
    - Celery workers (4x)
    - Whisper (audio)
    - Image processing
    - PDF extraction
```

---

### EK41 (Proxmox Host)

**VMs:**
```yaml
VM 1: Database
  - vCPU: 16 cores
  - RAM: 64GB
  - Disk: 500GB (RAID 1)
  - Services:
    - PostgreSQL 16
    - pgvector
    - pg_trgm (full-text)

VM 2: Cache & Queue
  - vCPU: 8 cores
  - RAM: 32GB
  - Disk: 200GB
  - Services:
    - Redis 7 (primary)
    - Redis Sentinel

VM 3: Storage & Backup
  - vCPU: 4 cores
  - RAM: 16GB
  - Disk: 800GB
  - Services:
    - MinIO
    - restic backup
    - Monitoring (Prometheus/Grafana)
```

---

## üöÄ ROADMAP DE IMPLEMENTA√á√ÉO

### Fase 1: Setup Infraestrutura (2-3 dias)
**Objetivo:** Preparar servidores e VMs

**Tarefas:**
- [ ] Configurar VMs no Proxmox (AX102 #1, #2, EK41)
- [ ] Setup rede vSwitch entre todos os servidores
- [ ] Instalar Docker em todas as VMs
- [ ] Configurar Nginx no AX102 #1
- [ ] Setup PostgreSQL no EK41
- [ ] Setup Redis no EK41
- [ ] Setup MinIO no EK41
- [ ] Testar conectividade entre todos os servi√ßos

---

### Fase 2: GPU Inference Server (1-2 dias)
**Objetivo:** Configurar vLLM com multi-modelo

**Tarefas:**
- [ ] Instalar vLLM no GPU server
- [ ] Download e quantizar modelos:
  - Llama 3.1 70B (AWQ 4-bit)
  - Mistral 7B Instruct
  - Phi-3 Medium
  - CodeLlama 34B
- [ ] Configurar vLLM multi-model serving
- [ ] Testar API OpenAI-compatible
- [ ] Configurar model router
- [ ] Setup monitoring (nvidia_gpu_exporter)

---

### Fase 3: Backend Gateway (2-3 dias)
**Objetivo:** API Node.js com tRPC

**Tarefas:**
- [ ] Criar projeto Node.js + TypeScript
- [ ] Setup Express + tRPC
- [ ] Implementar JWT authentication
- [ ] Criar endpoints:
  - POST /api/auth/login
  - POST /api/auth/register
  - POST /api/chat
  - WS /api/chat/stream
  - POST /api/files/upload
  - GET /api/conversations
- [ ] Integrar com PostgreSQL (Drizzle ORM)
- [ ] Integrar com Redis (cache)
- [ ] Setup Socket.io para streaming
- [ ] Testes unit√°rios

---

### Fase 4: AI Service (3-4 dias)
**Objetivo:** Python FastAPI com LangChain

**Tarefas:**
- [ ] Criar projeto Python + FastAPI
- [ ] Setup LangChain
- [ ] Implementar model router:
  - OpenAI GPT-4
  - Anthropic Claude
  - Local vLLM models
- [ ] Implementar RAG:
  - pgvector embeddings
  - Semantic search
  - Context builder
- [ ] Implementar Agent Loop:
  - Task planner
  - Tool executor
  - Retry logic
- [ ] Integrar com GPU inference (vLLM)
- [ ] Testes unit√°rios

---

### Fase 5: Processing Workers (2 dias)
**Objetivo:** Celery workers para tarefas ass√≠ncronas

**Tarefas:**
- [ ] Setup Celery + Redis
- [ ] Implementar workers:
  - Audio transcription (Whisper)
  - Image processing (Pillow)
  - PDF extraction (PyPDF2)
  - Web scraping (BeautifulSoup)
- [ ] Queue management
- [ ] Error handling e retry
- [ ] Monitoring

---

### Fase 6: Integra√ß√£o Frontend (2-3 dias)
**Objetivo:** Conectar page-navigator ao backend

**Tarefas:**
- [ ] Atualizar ChatArea.tsx (remover mock)
- [ ] Integrar com API Gateway (tRPC client)
- [ ] Implementar WebSocket streaming
- [ ] Integrar file upload com MinIO
- [ ] Integrar audio recording com Whisper
- [ ] Testar modo agente
- [ ] Ajustes de UI/UX

---

### Fase 7: Deployment & Testing (2-3 dias)
**Objetivo:** Deploy em produ√ß√£o e testes

**Tarefas:**
- [ ] Build frontend (Vite)
- [ ] Deploy frontend (VCloud/Vercel)
- [ ] Configure Nginx SSL (Let's Encrypt)
- [ ] Setup PM2 para backend services
- [ ] Configure systemd services
- [ ] Setup Prometheus + Grafana
- [ ] Testes end-to-end
- [ ] Load testing
- [ ] Security audit

---

### Fase 8: Documenta√ß√£o (1 dia)
**Objetivo:** Documentar tudo

**Tarefas:**
- [ ] README.md completo
- [ ] API documentation (Swagger)
- [ ] Deployment guide
- [ ] Troubleshooting guide
- [ ] Architecture diagrams
- [ ] User manual

---

## üìä ESTIMATIVA DE TEMPO

| Fase | Dura√ß√£o | Depend√™ncias |
|------|---------|--------------|
| 1. Infraestrutura | 2-3 dias | Nenhuma |
| 2. GPU Inference | 1-2 dias | Fase 1 |
| 3. Backend Gateway | 2-3 dias | Fase 1 |
| 4. AI Service | 3-4 dias | Fase 2, 3 |
| 5. Processing Workers | 2 dias | Fase 1, 4 |
| 6. Frontend Integration | 2-3 dias | Fase 3, 4 |
| 7. Deployment | 2-3 dias | Todas |
| 8. Documenta√ß√£o | 1 dia | Todas |
| **Total** | **15-21 dias** | |

**Com trabalho paralelo:** ~10-14 dias

---

## üí∞ CUSTOS ESTIMADOS

### Infraestrutura (J√° Existente)
- GPU Server (Hetzner): ~‚Ç¨150/m√™s
- 2x AX102 (Hetzner): ~‚Ç¨200/m√™s
- EK41 (Hetzner): ~‚Ç¨100/m√™s
- VCloud: A definir
- **Total:** ~‚Ç¨450/m√™s

### Servi√ßos Externos (Opcional)
- OpenAI API: Pay-as-you-go (~‚Ç¨50-200/m√™s)
- Anthropic API: Pay-as-you-go (~‚Ç¨50-200/m√™s)
- Cloudflare Pro: ‚Ç¨20/m√™s
- **Total:** ~‚Ç¨120-420/m√™s

### Total Mensal: ‚Ç¨570-870/m√™s

**Nota:** Modelos locais (vLLM) reduzem custos de API em 90%+

---

## üéØ PR√ìXIMOS PASSOS IMEDIATOS

1. ‚úÖ GPU limpa e pronta (1.6TB livre)
2. ‚è≥ Verificar IPs dos AX102 no Proxmox
3. ‚è≥ Verificar estado do EK41
4. ‚è≥ Decidir stack (Node.js puro ou H√≠brido)
5. ‚è≥ Come√ßar Fase 1 (Setup Infraestrutura)

---

**Aguardando decis√£o do utilizador para come√ßar implementa√ß√£o! üöÄ**
